{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Universidad de Buenos Aires\n",
        "# Aprendizaje Profundo - TP2\n",
        "# Cohorte 18 - 1er bimestre 2025\n"
      ],
      "metadata": {
        "id": "tHbzg4F1fLo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este segundo TP comienza desde el día después de la clase 5 (2 de abril) y la ventana de entrega estará abierta hasta las **23hs del miércoles 23 de abril (hora de Argentina)**. La resolución del TP es **individual**. Pueden utilizar los contenidos vistos en clase y otra bibliografía. Si se toman ideas de fuentes externas deben ser correctamente citadas incluyendo el correspondiente link o página de libro.\n",
        "\n",
        "El formato de entrega debe ser un link a un notebook de google colab (permitir acceso a gerardo.vilcamiza@ieee.org y gvilcamiza.ext@fi.uba.ar) y **se realizará en el siguiente link de google forms: [link](https://forms.gle/XatA691so4eVxZB68)**. Tanto los resultados, como el código y las explicaciones deben quedar guardados y visualizables en el colab.\n",
        "\n",
        "NO ES NECESARIO QUE NOS ENVIEN COREEO AVISANDO DE LA ENTREGA.\n",
        "\n",
        "**Consideraciones a tener en cuenta:**\n",
        "- Se entregará 1 solo colab para este TP2.\n",
        "- Renombrar el archivo de la siguiente manera: **APELLIDO-NOMBRE-DL-TP2-Co18.ipynb**\n",
        "- Los códigos deben poder ejecutarse.\n",
        "- Los resultados, cómo el código, los gráficos y las explicaciones deben quedar guardados y visualizables en el correspondiente notebook."
      ],
      "metadata": {
        "id": "PEib4WVwfQYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLASIFICADOR DE EMOCIONES**"
      ],
      "metadata": {
        "id": "bdseNqG3m7xX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo de este trabajo es construir una red neuronal convolucional (CNN) utilizando Pytorch, capaz de clasificar emociones humanas a partir de imágenes faciales. El clasificador deberá identificar una de las 7 emociones básicas: alegría, tristeza, enojo, miedo, sorpresa, disgusto y seriedad. El dataset se encuentra en este link: https://drive.google.com/file/d/10EWwOriegjawQ3evl1nuIldY3pUR-hQc/view?usp=sharing"
      ],
      "metadata": {
        "id": "u8jyqDP8bom6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Preprocesamiento de Datos (2 puntos)\n",
        "\n",
        "Antes de entrenar el modelo, se debe analizar qué tipo de preprocesamiento se debe aplicar a las imágenes. Para esto, se puede considerar uno o más aspectos como:\n",
        "\n",
        "- Tamaño\n",
        "- Relación de aspecto\n",
        "- Color o escala de grises\n",
        "- Cambio de dimensionalidad\n",
        "- Normalización\n",
        "- Balanceo de datos\n",
        "- Data augmentation\n",
        "- etc.\n",
        "\n",
        "Sean criteriosos y elijan las técnicas que consideren pertinentes.\n",
        "\n",
        "Recomendación: usar `torchvision.transforms` para facilitar el preprocesamiento.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y-ouGrVnbp7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Construcción y entrenamiento del Modelo CNN (3.5 puntos)\n",
        "\n",
        "- Construir una red neuronal convolucional desde cero, sin usar modelos pre-entrenados.\n",
        "- Analizar correctamente qué funciones de activación se deben usar en cada etapa de la red, el learning rate a utilizar, la función de costo y el optimizador.\n",
        "- Cosas como el número de capas, neuronas, tanaño de kernel, entre otros, queda a criterio de ustedes."
      ],
      "metadata": {
        "id": "Hk6B2VUvdufx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Evaluación del Modelo (3.5 puntos)\n",
        "\n",
        "El modelo entrenado debe ser evaluado utilizando las siguientes métricas:\n",
        "\n",
        "- **Accuracy**:\n",
        "  - Reportar el valor final en el conjunto de test.\n",
        "  - Incluir una gráfica de evolución por época para entrenamiento y validación.\n",
        "\n",
        "- **F1 Score**:\n",
        "  - Reportar el valor final en el conjunto de test.\n",
        "  - Incluir una gráfica de evolución por época para entrenamiento y validación.\n",
        "\n",
        "- **Costo (Loss)**:\n",
        "  - Mostrar una gráfica de evolución del costo por época para entrenamiento y validación.\n",
        "\n",
        "- **Classification report**\n",
        "  - Mostrar la precisión, recall y F1 score por cada clase usando `classification_report`\n",
        "\n",
        "- **Matriz de confusión**:\n",
        "  - Mostrar la matriz de confusión absoluta (valores enteros).\n",
        "  - Mostrar la matriz de confusión normalizada (valores entre 0 y 1 por fila).\n",
        "\n",
        "Se recomienda utilizar `scikit-learn` para calcular métricas como accuracy, F1 score, el Classification report y las matrices de confusión. Las visualizaciones pueden realizarse con `matplotlib` o `seaborn`, separando claramente los datos de entrenamiento y validación en las gráficas.\n"
      ],
      "metadata": {
        "id": "K5D3EvVRd-Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 4. Prueba con Imágenes Nuevas (1 punto)\n",
        "Subir al menos 3 imágenes personales (con el rostro de ustedes o de otras personas), que no formen parte del dataset de entrenamiento ni de testeo.\n",
        "\n",
        "- Cada imagen debe representar una emoción distinta.\n",
        "\n",
        "- Aplicar el mismo preprocesamiento que se usó para el dataset.\n",
        "\n",
        "- Pasar las imágenes por el modelo entrenado y mostrar:\n",
        "\n",
        "  - La imagen original (preprocesada)\n",
        "\n",
        "  - La clase inferida por el modelo\n",
        "\n",
        "- Redactar conclusiones y comentarios finales"
      ],
      "metadata": {
        "id": "40tsslqLgFlk"
      }
    }
  ]
}